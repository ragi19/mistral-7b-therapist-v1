{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "57885ebd-7db6-46cd-aadf-a4602170fa03",
    "_uuid": "3f5e90a7-5a2b-460a-a582-9a0c5074054a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Advanced Fine-Tuning of Mistral-7B for Mental Health Counseling\n",
    "\n",
    "![Mistral LLM Architecture](https://media.licdn.com/dms/image/v2/D4E12AQEgdey3xglFOA/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1702370058178?e=2147483647&v=beta&t=OeUF9dpRjZKMVvzIQ1ByoYK2i5nUep0qjidl9gn2nWo)\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates state-of-the-art fine-tuning of the Mistral-7B-Instruct-v0.1 model for mental health counseling applications. The implementation showcases advanced techniques in model adaptation while maintaining efficiency and performance.\n",
    "\n",
    "### Core Technologies\n",
    "- **Mistral-7B Base Model**: A powerful open-source language model with 7 billion parameters\n",
    "- **LoRA Fine-tuning**: Parameter-efficient adaptation technique\n",
    "- **4-bit Quantization**: Memory-efficient model deployment\n",
    "- **Hugging Face Integration**: Seamless model distribution\n",
    "- **Weights & Biases**: Comprehensive experiment tracking\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.10+\n",
    "- GPU with CUDA support\n",
    "- 24GB+ GPU memory recommended\n",
    "- Hugging Face account\n",
    "- Weights & Biases account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2b4f4470-a105-4e8d-b210-a888a764f223",
    "_uuid": "bec13480-aeac-419e-92c3-88b8ca7f0246",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Technical Implementation Details\n",
    "\n",
    "### 1. Model Architecture\n",
    "- **Base Model**: Mistral-7B-Instruct-v0.1\n",
    "- **Quantization**: 4-bit NF4 format with double quantization\n",
    "- **LoRA Configuration**: \n",
    "  - Rank: 16\n",
    "  - Alpha: 32\n",
    "  - Target modules: Query, Key, Value projections\n",
    "\n",
    "### 2. Training Configuration\n",
    "- **Hardware Requirements**:\n",
    "  - GPU: NVIDIA with 24GB+ VRAM\n",
    "  - CPU: 8+ cores recommended\n",
    "  - RAM: 32GB+ recommended\n",
    "\n",
    "- **Hyperparameters**:\n",
    "  - Learning rate: 2e-4\n",
    "  - Batch size: 1 (effective 4 with gradient accumulation)\n",
    "  - Training epochs: 3\n",
    "  - Max sequence length: 256 tokens\n",
    "\n",
    "### 3. Resource Management\n",
    "- **Memory Optimization**:\n",
    "  - 4-bit quantization\n",
    "  - Gradient checkpointing\n",
    "  - Efficient caching\n",
    "\n",
    "- **Performance Monitoring**:\n",
    "  - Real-time resource tracking\n",
    "  - GPU memory utilization\n",
    "  - Training metrics logging\n",
    "\n",
    "### 4. Dataset Processing\n",
    "- **Source**: Mental health counseling conversations dataset\n",
    "- **Format**: Structured dialogue pairs (user-therapist)\n",
    "- **Preprocessing**: \n",
    "  - Chat template formatting\n",
    "  - Token length normalization\n",
    "  - Quality filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "488c825d-fbc0-4c71-baa5-7c1a253c53d9",
    "_uuid": "c5eacc4c-091d-4863-8247-a2c8f7ac389d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Code Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b2301b11-91ba-49d7-a6c2-ea357bfed48d",
    "_uuid": "931be213-303e-423b-9fe1-6dce5b2c31a9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 1. Environment Setup\n",
    "```python\n",
    "%%capture\n",
    "%pip install -U transformers datasets accelerate peft trl bitsandbytes wandb evaluate nvidia-ml-py3\n",
    "```\n",
    "- Installs required libraries for model fine-tuning, monitoring, and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cd878bdd-5037-4c5d-9fac-47f6a8cbf705",
    "_uuid": "9ad81827-b782-4c1b-9b1e-d21ca8ff46bd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-10T23:25:30.936678Z",
     "iopub.status.busy": "2025-02-10T23:25:30.936318Z",
     "iopub.status.idle": "2025-02-10T23:26:36.445295Z",
     "shell.execute_reply": "2025-02-10T23:26:36.444226Z",
     "shell.execute_reply.started": "2025-02-10T23:25:30.936652Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers \n",
    "%pip install -U datasets \n",
    "%pip install -U accelerate \n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install -U bitsandbytes \n",
    "%pip install -U wandb\n",
    "%pip install evaluate\n",
    "%pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "30f55f6c-ae98-43ec-8967-b3a56536fda4",
    "_uuid": "f5e6cfa4-ebf3-4866-8724-23810f4909be",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7395a465-cfe2-4e35-9bc6-7956dbd89b5a",
    "_uuid": "0d4b03b7-dd00-46be-8aca-e7ed10a169cf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-10T23:26:36.447011Z",
     "iopub.status.busy": "2025-02-10T23:26:36.446696Z",
     "iopub.status.idle": "2025-02-10T23:27:01.332503Z",
     "shell.execute_reply": "2025-02-10T23:27:01.331724Z",
     "shell.execute_reply.started": "2025-02-10T23:26:36.446971Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import logging  # Standard Python logging\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import wandb\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1aeca861-84f2-4795-9af5-2aaf23a704e1",
    "_uuid": "985c27b4-b5f4-4454-b3de-595d601bba36",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "- Imports necessary libraries for model handling, dataset processing, resource monitoring, and logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8c66b92-37cd-472c-a12b-309aec2799a3",
    "_uuid": "83b12c0d-24c9-49e7-8109-fbbfd0ec1bea",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 3. Resource Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "381fd456-f14f-43cf-90a3-a7b4217a22ca",
    "_uuid": "27cdf2c0-c0d9-4da8-a61e-7b112b0bfd6d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-10T23:28:56.452425Z",
     "iopub.status.busy": "2025-02-10T23:28:56.452063Z",
     "iopub.status.idle": "2025-02-10T23:28:56.463022Z",
     "shell.execute_reply": "2025-02-10T23:28:56.462188Z",
     "shell.execute_reply.started": "2025-02-10T23:28:56.452398Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResourceMonitor:\n",
    "    def __init__(self, interval=1):\n",
    "        self.interval = interval\n",
    "        self.running = False\n",
    "        self.stats = []\n",
    "        \n",
    "    def start(self):\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._monitor)\n",
    "        self.thread.start()\n",
    "        \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        self.thread.join()\n",
    "        \n",
    "    def _monitor(self):\n",
    "        gpu_utilization = None\n",
    "        while self.running:\n",
    "            gpu_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "            \n",
    "            # Attempt to get GPU utilization\n",
    "            if torch.cuda.is_available():\n",
    "                try:\n",
    "                    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetUtilizationRates, nvmlShutdown\n",
    "                    nvmlInit()\n",
    "                    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "                    gpu_utilization = nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "                    nvmlShutdown()\n",
    "                except ModuleNotFoundError:\n",
    "                    gpu_utilization = \"pynvml not available\"\n",
    "                except Exception as e:\n",
    "                    gpu_utilization = f\"Error: {str(e)}\"\n",
    "            \n",
    "            self.stats.append({\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'cpu_percent': psutil.cpu_percent(),\n",
    "                'ram_percent': psutil.virtual_memory().percent,\n",
    "                'gpu_memory_gb': gpu_memory / (1024**3),\n",
    "                'gpu_utilization': gpu_utilization or 0\n",
    "            })\n",
    "            time.sleep(self.interval)\n",
    "\n",
    "def get_report(self):\n",
    "    if not self.stats:\n",
    "        return \"No monitoring data available\"\n",
    "    \n",
    "    stats_array = np.array([(s['cpu_percent'], s['ram_percent'], s['gpu_memory_gb'], s['gpu_utilization']) \n",
    "                           for s in self.stats])\n",
    "    \n",
    "    return {\n",
    "        'cpu_percent': {\n",
    "            'mean': np.mean(stats_array[:, 0]),\n",
    "            'max': np.max(stats_array[:, 0])\n",
    "        },\n",
    "        'ram_percent': {\n",
    "            'mean': np.mean(stats_array[:, 1]),\n",
    "            'max': np.max(stats_array[:, 1])\n",
    "        },\n",
    "        'gpu_memory_gb': {\n",
    "            'mean': np.mean(stats_array[:, 2]),\n",
    "            'max': np.max(stats_array[:, 2])\n",
    "        },\n",
    "        'gpu_utilization': {\n",
    "            'mean': np.mean(stats_array[:, 3]),\n",
    "            'max': np.max(stats_array[:, 3])\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b41888d1-057a-42a7-8181-e6d8e347ad01",
    "_uuid": "b760d4e7-968e-41e7-9544-73f2907aeb94",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "- Implements a thread-based monitor for tracking system resources during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e3051424-ea2b-4136-b885-de6a97f77e12",
    "_uuid": "dfd740c6-ae76-48be-a8dd-d59a2f97498f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 4. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e453ef4f-9e92-4294-a01e-e06fd789adaf",
    "_uuid": "70982a1f-6b47-483f-95aa-018fd17023f9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-10T23:28:59.154695Z",
     "iopub.status.busy": "2025-02-10T23:28:59.154362Z",
     "iopub.status.idle": "2025-02-10T23:28:59.163144Z",
     "shell.execute_reply": "2025-02-10T23:28:59.162005Z",
     "shell.execute_reply.started": "2025-02-10T23:28:59.154664Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict], tokenizer, max_length: int = 256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Format conversation using chat template\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": item[\"Context\"]},\n",
    "            {\"role\": \"assistant\", \"content\": item[\"Response\"]}\n",
    "        ]\n",
    "        \n",
    "        # Apply tokenizer's chat template\n",
    "        text = self.tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "        \n",
    "        # Tokenize\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': encodings['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "\n",
    "def setup_logging(log_file=\"training_log.txt\"):\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,  # Set the logging level to INFO\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),  # Log to a file\n",
    "            logging.StreamHandler()  # Also log to console\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger()  # Return the logger instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7245094e-3279-4c37-aad6-1a381edcfc97",
    "_uuid": "09cf0830-a445-422e-8c5f-825525c1199f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 5. Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d11b4065-53be-4e9e-8cee-9d0872005973",
    "_uuid": "ee15ecb9-ce15-4269-afe4-7834fad14dca",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-10T23:29:02.610943Z",
     "iopub.status.busy": "2025-02-10T23:29:02.610566Z",
     "iopub.status.idle": "2025-02-10T23:29:02.617384Z",
     "shell.execute_reply": "2025-02-10T23:29:02.616386Z",
     "shell.execute_reply.started": "2025-02-10T23:29:02.610912Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_model_and_tokenizer(base_model_path: str, device_map: str = \"auto\"):\n",
    "    # Configure 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    # Load and configure tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7acf2b22-aa6a-4acc-b6cc-1c81c3443569",
    "_uuid": "bb40c780-f32c-4350-91e8-d873698b5c9c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 6. LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a69a73ae-0f1e-4dd4-b836-5fccd3b8d526",
    "_uuid": "03611ee6-9682-4dbd-9976-4010820b7a83",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-10T23:29:05.377366Z",
     "iopub.status.busy": "2025-02-10T23:29:05.376990Z",
     "iopub.status.idle": "2025-02-10T23:29:05.383439Z",
     "shell.execute_reply": "2025-02-10T23:29:05.382285Z",
     "shell.execute_reply.started": "2025-02-10T23:29:05.377337Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def configure_lora(model, r=16, alpha=32):\n",
    "    config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        lora_dropout=0.01,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "    return get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "801fa3c0-7596-4f4f-ac3a-f6cc983d5171",
    "_uuid": "a38784aa-e813-4c6e-98fe-9f8e30fe2cf2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "| Component | Configuration |\n",
    "|-----------|---------------|\n",
    "| Quantization | NF4 with double quantization |\n",
    "| LoRA Rank | 16 |\n",
    "| Batch Size | 1 (Effective 4 via grad accumulation) |\n",
    "| Learning Rate | 2e-4 |\n",
    "| Context Length | 256 tokens |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fba33b18-6eff-43ff-8da6-4c5f1bb4d091",
    "_uuid": "ba586027-44c4-4825-a862-22cbb41c0757",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 7. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ac0e0365-6605-45b8-ab0d-9e5f74b6c0e4",
    "_uuid": "412e8471-f6c4-4958-99ac-94b84fcac012",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-10T23:29:08.322024Z",
     "iopub.status.busy": "2025-02-10T23:29:08.321660Z",
     "iopub.status.idle": "2025-02-10T23:29:09.538127Z",
     "shell.execute_reply": "2025-02-10T23:29:09.537141Z",
     "shell.execute_reply.started": "2025-02-10T23:29:08.321996Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource stats collected.\n",
      "Authentication completed\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import gc\n",
    "import psutil  # System resource monitoring\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import HfApi, Repository\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Resource monitoring using psutil\n",
    "cpu_usage = psutil.cpu_percent(interval=1)\n",
    "memory_usage = psutil.virtual_memory().percent\n",
    "disk_usage = psutil.disk_usage('/').percent\n",
    "gpu_usage = \"Not available\"  # Placeholder, integrate nvidia-smi if needed\n",
    "\n",
    "resource_stats = {\n",
    "    \"cpu\": cpu_usage,\n",
    "    \"memory\": memory_usage,\n",
    "    \"disk\": disk_usage,\n",
    "    \"gpu\": gpu_usage\n",
    "}\n",
    "print(\"Resource stats collected.\")\n",
    "\n",
    "# Authentication\n",
    "user_secrets = UserSecretsClient()\n",
    "login(token=user_secrets.get_secret(\"mental\"))\n",
    "wandb.login(key=user_secrets.get_secret(\"wandb\"))\n",
    "print(\"Authentication completed\")\n",
    "\n",
    "run = wandb.init(\n",
    "    project='Advanced-Mistral-7B-Mental-Health',\n",
    "    job_type=\"training\",\n",
    "    config={\n",
    "        \"model_name\": \"mistral-7b-instruct-v0.1\",\n",
    "        \"dataset\": \"mental_health_counseling_conversations\",\n",
    "        \"batch_size\": 1,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"epochs\": 3,\n",
    "        \"max_length\": 256\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "890638c4-057b-45b9-ae67-f932d592ee31",
    "_uuid": "77213f6b-7725-4fa6-a800-44fd92a1669f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-10T23:29:13.214627Z",
     "iopub.status.busy": "2025-02-10T23:29:13.214292Z",
     "iopub.status.idle": "2025-02-10T23:30:32.582169Z",
     "shell.execute_reply": "2025-02-10T23:30:32.581092Z",
     "shell.execute_reply.started": "2025-02-10T23:29:13.214601Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bcb54fca7c4660b0b7795342cece63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1895220c32dd4f61b998f4dc2687ce75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f8cb1a4f9e477881d01272e36222e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "combined_dataset.json:   0%|          | 0.00/4.79M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb5afeecca54fa991347a5396a7deb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model, tokenizer = prepare_model_and_tokenizer(\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\")\n",
    "model = configure_lora(model)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "dataset = load_dataset(\"Amod/mental_health_counseling_conversations\", split=\"all\")\n",
    "dataset = dataset.shuffle(seed=42).select(range(min(2000, len(dataset))))\n",
    "train_val_split = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = CustomDataset(train_val_split[\"train\"], tokenizer)\n",
    "eval_dataset = CustomDataset(train_val_split[\"test\"], tokenizer)\n",
    "print(\"Dataset prepared.\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral-7b-therapist-v2\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cf32a705-e9b6-4e3e-9af0-9607b86716ef",
    "_uuid": "57592e53-832e-4f13-87a7-567fff9a9e95",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-10T23:36:16.829964Z",
     "iopub.status.busy": "2025-02-10T23:36:16.829613Z",
     "iopub.status.idle": "2025-02-11T02:18:41.609004Z",
     "shell.execute_reply": "2025-02-11T02:18:41.608311Z",
     "shell.execute_reply.started": "2025-02-10T23:36:16.829940Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-49da3de37cdc>:2: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1350' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1350/1350 2:42:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.995200</td>\n",
       "      <td>2.017891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.015000</td>\n",
       "      <td>1.832786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.806400</td>\n",
       "      <td>1.699195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.602600</td>\n",
       "      <td>1.610040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.858300</td>\n",
       "      <td>1.533294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>1.472184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.781900</td>\n",
       "      <td>1.394812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.764900</td>\n",
       "      <td>1.329779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>1.236731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.248600</td>\n",
       "      <td>1.356432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.302700</td>\n",
       "      <td>1.329080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.255600</td>\n",
       "      <td>1.306612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>1.299989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at ./mistral-7b-therapist-v2/checkpoint-900/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "20314a81-d7bf-4131-bba7-b634fa8069eb",
    "_uuid": "b7189204-0d4f-4e0a-bc16-2e83e9ab5780",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Best Practices and Recommendations\n",
    "\n",
    "### 1. Training Process\n",
    "- Monitor training loss and validation metrics closely\n",
    "- Use early stopping if validation loss plateaus\n",
    "- Save checkpoints regularly\n",
    "- Track resource utilization\n",
    "\n",
    "### 2. Model Deployment\n",
    "- Test model throughput and latency\n",
    "- Implement proper error handling\n",
    "- Set up monitoring for production use\n",
    "- Consider model versioning\n",
    "\n",
    "### 3. Ethical Considerations\n",
    "- Ensure responsible AI practices\n",
    "- Monitor for biased responses\n",
    "- Implement content filtering\n",
    "- Regular model evaluation\n",
    "\n",
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues\n",
    "1. **Out of Memory (OOM)**:\n",
    "   - Reduce batch size\n",
    "   - Enable gradient checkpointing\n",
    "   - Increase quantization level\n",
    "\n",
    "2. **Training Instability**:\n",
    "   - Adjust learning rate\n",
    "   - Check for data quality issues\n",
    "   - Monitor gradient norms\n",
    "\n",
    "3. **Poor Performance**:\n",
    "   - Validate dataset quality\n",
    "   - Review hyperparameters\n",
    "   - Check for overfitting\n",
    "\n",
    "## References\n",
    "- [Mistral AI Documentation](https://docs.mistral.ai/)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [Quantization Techniques](https://arxiv.org/abs/2208.07339)\n",
    "- [Mental Health Counseling Best Practices](https://www.who.int/mental_health/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "800a2700-4aa9-4265-aa6b-ed470be0bd71",
    "_uuid": "5daae8eb-66a8-4a05-9c7b-2e336ba9a2dd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-11T02:22:27.607338Z",
     "iopub.status.busy": "2025-02-11T02:22:27.606932Z",
     "iopub.status.idle": "2025-02-11T02:24:07.586569Z",
     "shell.execute_reply": "2025-02-11T02:24:07.585689Z",
     "shell.execute_reply.started": "2025-02-11T02:22:27.607309Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 01:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation completed.\n",
      "Model saved locally.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "metrics = trainer.evaluate()\n",
    "eval_performance = {\n",
    "    \"eval_loss\": metrics.get(\"eval_loss\"),\n",
    "    \"eval_accuracy\": metrics.get(\"eval_accuracy\", \"Not available\"),\n",
    "    \"eval_f1\": metrics.get(\"eval_f1\", \"Not available\")\n",
    "}\n",
    "print(\"Model evaluation completed.\")\n",
    "# Save and push model to Hugging Face\n",
    "model_dir = \"./mistral-7b-therapist-v2\"\n",
    "trainer.model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "print(\"Model saved locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T02:24:15.549887Z",
     "iopub.status.busy": "2025-02-11T02:24:15.549546Z",
     "iopub.status.idle": "2025-02-11T02:24:30.070030Z",
     "shell.execute_reply": "2025-02-11T02:24:30.069149Z",
     "shell.execute_reply.started": "2025-02-11T02:24:15.549855Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613e74f1f61b41f4b5fd427c7251f5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c6cc626b2a4d1e8a99928f833b30f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer pushed to Hugging Face under the repository: mistral-7b-therapist-v1\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>eval/runtime</td><td>‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.29708</td></tr><tr><td>eval/runtime</td><td>99.6254</td></tr><tr><td>eval/samples_per_second</td><td>2.008</td></tr><tr><td>eval/steps_per_second</td><td>0.251</td></tr><tr><td>total_flos</td><td>5.93265514512384e+16</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>1350</td></tr><tr><td>train/grad_norm</td><td>2.35212</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.2211</td></tr><tr><td>train_loss</td><td>0.98771</td></tr><tr><td>train_runtime</td><td>9744.2589</td></tr><tr><td>train_samples_per_second</td><td>0.554</td></tr><tr><td>train_steps_per_second</td><td>0.139</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neat-eon-24</strong> at: <a href='https://wandb.ai/ragishehab/Advanced-Mistral-7B-Mental-Health/runs/9v60xpmy' target=\"_blank\">https://wandb.ai/ragishehab/Advanced-Mistral-7B-Mental-Health/runs/9v60xpmy</a><br> View project at: <a href='https://wandb.ai/ragishehab/Advanced-Mistral-7B-Mental-Health' target=\"_blank\">https://wandb.ai/ragishehab/Advanced-Mistral-7B-Mental-Health</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250210_232821-9v60xpmy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training process completed\n"
     ]
    }
   ],
   "source": [
    "login(token=user_secrets.get_secret(\"mental\"))\n",
    "repo_name = \"mistral-7b-therapist-v1\"\n",
    "trainer.model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "print(f\"Model and tokenizer pushed to Hugging Face under the repository: {repo_name}\")\n",
    "\n",
    "# Cleanup\n",
    "wandb.finish()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Training process completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "447322bd-3f48-4457-9a34-625e964ee2c5",
    "_uuid": "37af1626-c14c-44ad-9992-2c2c21a68c75",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "- Saves the fine-tuned model and tokenizer locally and pushes them to Hugging Face's model hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0215536d-621a-49cb-afb9-01b761d6ea6e",
    "_uuid": "2ed718aa-43b7-410c-84d5-23dec55a8c27",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Conclusion\n",
    "This notebook provides a comprehensive workflow for fine-tuning a large language model for specialized applications like mental health counseling. By leveraging advanced techniques such as quantization, LoRA, and resource monitoring, it ensures efficient and effective model adaptation while maintaining high performance."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "modelId": 1902,
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
